{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba551af",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de25f82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d66f73d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Porter Stemmer object\n",
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8d12f4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running -> run\n",
      "ran -> ran\n",
      "jumps -> jump\n",
      "jumped -> jump\n",
      "happiness -> happi\n",
      "happy -> happi\n"
     ]
    }
   ],
   "source": [
    "words = [\"running\", \"ran\", \"jumps\", \"jumped\", \"happiness\", \"happy\"]\n",
    "\n",
    "stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
    "\n",
    "for original, stemmed in zip(words, stemmed_words):\n",
    "    print(f\"{original} -> {stemmed}\")\n",
    "    \n",
    "# The above for is same as the one below\n",
    "# for i in range(len(words)):\n",
    "#     print(f\"{words[i]} -> {stemmed_words[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518ffe66",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c88783c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a658ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97609f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural -> Natural\n",
      "Language -> Language\n",
      "Processing -> Processing\n",
      "( -> (\n",
      "NLP -> NLP\n",
      ") -> )\n",
      "is -> is\n",
      "a -> a\n",
      "subfield -> subfield\n",
      "of -> of\n",
      "artificial -> artificial\n",
      "intelligence -> intelligence\n",
      "that -> that\n",
      "focuses -> focus\n",
      "on -> on\n",
      "the -> the\n",
      "interaction -> interaction\n",
      "between -> between\n",
      "computers -> computer\n",
      "and -> and\n",
      "humans -> human\n",
      "using -> using\n",
      "natural -> natural\n",
      "language -> language\n",
      ". -> .\n",
      "It -> It\n",
      "involves -> involves\n",
      "the -> the\n",
      "development -> development\n",
      "of -> of\n",
      "algorithms -> algorithm\n",
      "and -> and\n",
      "models -> model\n",
      "that -> that\n",
      "enable -> enable\n",
      "machines -> machine\n",
      "to -> to\n",
      "understand -> understand\n",
      ", -> ,\n",
      "interpret -> interpret\n",
      ", -> ,\n",
      "and -> and\n",
      "generate -> generate\n",
      "human-like -> human-like\n",
      "text -> text\n",
      ". -> .\n",
      "NLP -> NLP\n",
      "plays -> play\n",
      "a -> a\n",
      "crucial -> crucial\n",
      "role -> role\n",
      "in -> in\n",
      "various -> various\n",
      "applications -> application\n",
      ", -> ,\n",
      "including -> including\n",
      "sentiment -> sentiment\n",
      "analysis -> analysis\n",
      ", -> ,\n",
      "machine -> machine\n",
      "translation -> translation\n",
      ", -> ,\n",
      "chatbots -> chatbots\n",
      ", -> ,\n",
      "and -> and\n",
      "information -> information\n",
      "extraction -> extraction\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"\"\"\n",
    "Natural Language Processing (NLP) is a subfield of artificial intelligence that focuses on the interaction \n",
    "between computers and humans using natural language. It involves the development of algorithms and models \n",
    "that enable machines to understand, interpret, and generate human-like text. NLP plays a crucial role in \n",
    "various applications, including sentiment analysis, machine translation, \n",
    "chatbots, and information extraction\n",
    "\"\"\"\n",
    "\n",
    "tokens = word_tokenize(paragraph)\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "for original, lemmatized in zip(tokens, lemmatized_tokens):\n",
    "    print(f\"{original} -> {lemmatized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383a3a84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
